{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 1: SETUP & CONFIGURATION ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "print(\"‚úì Core libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Polars display settings\n",
    "pl.Config.set_tbl_rows(25)\n",
    "pl.Config.set_tbl_cols(-1)\n",
    "pl.Config.set_tbl_width_chars(1000)\n",
    "\n",
    "print(\"‚úì Polars display configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Matplotlib configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"‚úì Autoreload enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"‚úÖ SETUP COMPLETE - Ready to load data\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 2: DATA LOADING & VALIDATION ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS Authentication (for Google Colab)\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    \n",
    "    import os\n",
    "    os.environ['GCLOUD_PROJECT'] = 'alpine-charge-404612'\n",
    "    \n",
    "    print(\"‚úì GCP authenticated\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  Not running in Colab - skipping GCP auth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS Configuration\n",
    "import tarfile\n",
    "import zipfile\n",
    "import shutil\n",
    "import traceback\n",
    "from google.cloud import storage\n",
    "from google.api_core import retry\n",
    "\n",
    "# Config\n",
    "BUCKET_NAME = \"spread-eth-oxford\"\n",
    "GCS_PREFIX = \"polydata/\"\n",
    "GCS_RAW_ARCHIVE = \"archive.tar.xz\"\n",
    "WORK_DIR = \"data\"\n",
    "\n",
    "# Initialize GCS with retry\n",
    "try:\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    print(f\"‚úì Connected to GCS bucket: {BUCKET_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  GCS connection failed: {e}\")\n",
    "    storage_client = None\n",
    "    bucket = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_disk_space(required_gb=100):\n",
    "    \"\"\"Check if we have enough disk space\"\"\"\n",
    "    disk = shutil.disk_usage('.')\n",
    "    free_gb = disk.free / (1024**3)\n",
    "\n",
    "    print(f\"üíæ Disk space check:\")\n",
    "    print(f\"   Free: {free_gb:.1f} GB\")\n",
    "    print(f\"   Required: ~{required_gb} GB\")\n",
    "\n",
    "    if free_gb < required_gb:\n",
    "        print(f\"\\n‚ùå ERROR: Not enough disk space!\")\n",
    "        print(f\"   Need {required_gb}GB, have {free_gb:.1f}GB\")\n",
    "        return False\n",
    "\n",
    "    print(f\"   ‚úì Sufficient space available\\n\")\n",
    "    return True\n",
    "\n",
    "def check_gcs_data():\n",
    "    \"\"\"Check if processed data exists in GCS\"\"\"\n",
    "    if not bucket:\n",
    "        return False\n",
    "    try:\n",
    "        blobs = [b for b in bucket.list_blobs(prefix=GCS_PREFIX) if b.name.endswith('.zip')]\n",
    "        return len(blobs) > 0\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Could not check GCS: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úì GCS helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_gcs():\n",
    "    \"\"\"Download and extract zip archives from GCS with retry\"\"\"\n",
    "    if not bucket:\n",
    "        print(\"‚ùå GCS not connected\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "        blobs = [b for b in bucket.list_blobs(prefix=GCS_PREFIX) if b.name.endswith('.zip')]\n",
    "        print(f\"üì• Downloading {len(blobs)} zip archive(s) from GCS...\\n\")\n",
    "\n",
    "        for i, blob in enumerate(blobs, 1):\n",
    "            filename = blob.name.split('/')[-1]\n",
    "            zip_path = filename\n",
    "\n",
    "            # Skip if already extracted\n",
    "            expected_files = Path(WORK_DIR).glob(f\"{filename.replace('.zip', '')}*.parquet\")\n",
    "            if any(expected_files):\n",
    "                print(f\"[{i}/{len(blobs)}] {filename} - Already extracted, skipping\")\n",
    "                continue\n",
    "\n",
    "            # Reload blob to get metadata\n",
    "            blob.reload()\n",
    "            blob_size_mb = (blob.size or 0) / (1024**2)\n",
    "            print(f\"[{i}/{len(blobs)}] {filename} ({blob_size_mb:.1f} MB)...\", end=\" \")\n",
    "\n",
    "            # Download with retry\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    start = time.time()\n",
    "                    blob.download_to_filename(zip_path)\n",
    "                    elapsed = time.time() - start\n",
    "                    file_size_mb = os.path.getsize(zip_path) / (1024**2)\n",
    "                    speed = file_size_mb / elapsed if elapsed > 0 else 0\n",
    "                    print(f\"‚úì ({speed:.1f} MB/s)\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        print(f\"\\n   Retry {attempt+1}/{max_retries}...\", end=\" \")\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        raise Exception(f\"Download failed after {max_retries} attempts: {e}\")\n",
    "\n",
    "            # Extract\n",
    "            print(f\"    Extracting...\", end=\" \")\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "                    zipf.extractall(WORK_DIR)\n",
    "                print(f\"‚úì\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå ERROR extracting {filename}: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Remove zip\n",
    "            os.remove(zip_path)\n",
    "            print()\n",
    "\n",
    "        print(f\"‚úÖ Downloaded and extracted to {WORK_DIR}/\\n\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR in download_from_gcs: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "print(\"‚úì download_from_gcs() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main GCS data loading execution\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ DATA LOADER\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "success = False\n",
    "\n",
    "if check_gcs_data():\n",
    "    print(\"‚úÖ Processed data found in GCS! Downloading...\\n\")\n",
    "    success = download_from_gcs()\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No processed data in GCS\")\n",
    "    print(\"   Place data files in data/ directory or run GCS upload process\")\n",
    "\n",
    "if success:\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚úÖ DATA READY!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Show files\n",
    "    print(f\"\\nFiles in {WORK_DIR}/:\")\n",
    "    parquet_files = sorted(Path(WORK_DIR).glob('*.parquet'))\n",
    "    total_size = 0\n",
    "    for f in parquet_files:\n",
    "        size_mb = f.stat().st_size / (1024**2)\n",
    "        total_size += size_mb\n",
    "        print(f\"  üìÑ {f.name} - {size_mb:.1f} MB\")\n",
    "\n",
    "    if parquet_files:\n",
    "        print(f\"\\nTotal: {len(parquet_files)} files, {total_size/1024:.2f} GB\")\n",
    "\n",
    "    disk = shutil.disk_usage('.')\n",
    "    print(f\"üíæ Disk: {disk.free/(1024**3):.1f} GB free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import poly_utils\n# Import poly_utils for missing token updates and platform wallets\n# Import poly_utils for optional CSV-based market updates and platform wallets\nfrom poly_utils.utils import get_markets, update_missing_tokens, PLATFORM_WALLETS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load markets using poly_utils\nprint(\"üìä Loading markets...\")\n\n# Load markets from parquet (downloaded from GCS)\nmarkets_parquet = Path(f\"{WORK_DIR}/markets.parquet\")\n\nif markets_parquet.exists():\n    markets_df = pl.read_parquet(markets_parquet)\n    print(f\"‚úì Markets loaded from parquet: {len(markets_df):,} markets\")\n    print(f\"   Volume range: ${markets_df['volume'].min():,.0f} - ${markets_df['volume'].max():,.0f}\")\n\n    # Check if datetime column needs parsing\n    if markets_df['createdAt'].dtype == pl.Utf8:\n        markets_df = markets_df.with_columns(\n            pl.col(\"createdAt\").str.to_datetime().alias(\"createdAt\")\n        )\nelse:\n    print(f\"‚ùå No markets data found\")\n    print(f\"   Expected: {markets_parquet}\")\n    print(f\"   Run data update pipeline in Section 4 to generate data\")\n    markets_df = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load trades from processed directory\nprint(\"üìà Loading trades...\")\n\n# Load trades from parquet files\ntrades_files = sorted(Path(WORK_DIR).glob(\"trades*.parquet\"))\n\nif len(trades_files) > 0:\n    print(f\"üìà Loading {len(trades_files)} trade file(s)...\")\n\n    if len(trades_files) == 1:\n        trades_df = pl.read_parquet(trades_files[0])\n    else:\n        # Multiple files - use lazy scanning for memory efficiency\n        print(\"   Using memory-efficient lazy loading...\")\n        trades_df = pl.concat([\n            pl.scan_parquet(f) for f in trades_files\n        ]).collect(streaming=True)\n\n    # Check if timestamp needs parsing\n    if trades_df['timestamp'].dtype == pl.Utf8:\n        trades_df = trades_df.with_columns(\n            pl.col(\"timestamp\").str.to_datetime().alias(\"timestamp\")\n        )\n\n    print(f\"‚úì Trades loaded: {len(trades_df):,} trades\")\n    print(f\"   Total volume: ${trades_df['usd_amount'].sum()/1e9:.2f}B\")\n    print(f\"   Date range: {trades_df['timestamp'].min()} to {trades_df['timestamp'].max()}\")\n    print(f\"   Unique markets: {trades_df['market_id'].n_unique():,}\")\nelse:\n    print(\"‚ùå No trades data found\")\n    print(f\"   Expected: {WORK_DIR}/trades*.parquet\")\n    print(f\"   Run data update pipeline in Section 4 to generate data\")\n    trades_df = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if markets_df is not None and trades_df is not None:\n    # Validation: Check for missing markets in trades\n    print(\"üîç Validating data...\")\n    \n    missing_market_ids = set(trades_df['market_id'].unique()) - set(markets_df['id'].unique())\n    \n    if missing_market_ids:\n        print(f\"‚ö†Ô∏è  Warning: {len(missing_market_ids)} market IDs in trades not found in markets\")\n        print(f\"   Sample missing IDs: {list(missing_market_ids)[:5]}\")\n        print(f\"   Consider running update_missing_tokens() in Section 3\")\n    else:\n        print(\"‚úÖ All trade markets found in markets dataset\")\nelse:\n    print(\"‚ö†Ô∏è  Cannot validate - data not loaded\")\n    missing_market_ids = set()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if trades_df is not None:\n    # Validation: Data freshness check\n    last_trade_time = trades_df['timestamp'].max()\n    hours_since_last = (datetime.now() - last_trade_time).total_seconds() / 3600\n    \n    print(f\"\\n‚è∞ Data freshness:\")\n    print(f\"   Last trade: {last_trade_time}\")\n    print(f\"   Hours ago: {hours_since_last:.1f}h\")\n    \n    if hours_since_last > 24:\n        print(\"   ‚ö†Ô∏è  Data is stale - consider running update pipeline in Section 4\")\n    else:\n        print(\"   ‚úÖ Data is fresh\")\n        \n    print(\"\\n\" + \"=\"*60)\n    print(\"‚úÖ DATA VALIDATION COMPLETE\")\n    print(\"=\"*60)\nelse:\n    print(\"‚ö†Ô∏è  Cannot check freshness - trades data not loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 3: POLYUTILS - MARKET MANAGEMENT ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display platform wallet addresses\n",
    "print(\"üíº Platform Wallets Tracked:\")\n",
    "for i, wallet in enumerate(PLATFORM_WALLETS, 1):\n",
    "    print(f\"   {i}. {wallet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if markets_df is not None:\n    # Example: Filter markets by volume and keyword\n    print(\"üîç Market Filtering Examples:\\n\")\n    \n    # High volume markets\n    high_volume = markets_df.filter(pl.col('volume') >= 1_000_000).sort('volume', descending=True)\n    print(f\"1. High volume markets (>=1M): {len(high_volume):,}\")\n    print(high_volume.select(['question', 'volume']).head(5))\n    \n    print(\"\\n\" + \"-\"*60 + \"\\n\")\n    \n    # Search by keyword\n    keyword = \"Trump\"\n    keyword_markets = markets_df.filter(\n        pl.col('question').str.contains(keyword, literal=False)\n    ).sort('volume', descending=True)\n    print(f\"2. Markets containing '{keyword}': {len(keyword_markets):,}\")\n    print(keyword_markets.select(['question', 'volume']).head(5))\nelse:\n    print(\"‚ö†Ô∏è  Markets data not loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Update missing tokens (COMMENTED OUT - run only when needed)\n",
    "print(\"üìù Update Missing Tokens Example:\")\n",
    "print(\"   Use this when you have market IDs in trades but not in markets dataset\")\n",
    "print(\"   Uncomment and run the code below:\\n\")\n",
    "\n",
    "print(\"   # missing_ids = ['market_id_1', 'market_id_2', ...]\")\n",
    "print(\"   # update_missing_tokens(missing_ids, output_file='missing_markets.csv')\")\n",
    "print(\"   # Then reload markets with get_markets()\")\n",
    "\n",
    "if missing_market_ids:\n",
    "    print(f\"\\n   üí° You currently have {len(missing_market_ids)} missing markets\")\n",
    "    print(f\"   Sample IDs to update: {list(missing_market_ids)[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display market schema\n",
    "print(\"üìã Market Data Schema:\")\n",
    "print(f\"   Total columns: {len(markets_df.columns)}\")\n",
    "print(f\"\\n   Columns: {', '.join(markets_df.columns)}\")\n",
    "print(f\"\\n   Sample row:\")\n",
    "print(markets_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 4: UPDATE UTILS - DATA PIPELINE ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import update_utils modules\n",
    "from update_utils.update_markets import update_markets\n",
    "from update_utils.update_goldsky import update_goldsky\n",
    "from update_utils.process_live import process_live\n",
    "\n",
    "print(\"‚úì update_utils modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wrapper function for complete data update\n",
    "def update_all_data():\n",
    "    \"\"\"\n",
    "    Run complete data pipeline update:\n",
    "    1. Update markets from Polymarket API\n",
    "    2. Scrape order events from Goldsky subgraph\n",
    "    3. Process raw orders into structured trades\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Starting data update pipeline...\\n\")\n",
    "    \n",
    "    try:\n",
    "        print(\"1Ô∏è‚É£ Updating markets from Polymarket API...\")\n",
    "        update_markets()\n",
    "        print(\"   ‚úì Markets updated\\n\")\n",
    "        \n",
    "        print(\"2Ô∏è‚É£ Scraping order events from Goldsky...\")\n",
    "        update_goldsky()\n",
    "        print(\"   ‚úì Orders scraped\\n\")\n",
    "        \n",
    "        print(\"3Ô∏è‚É£ Processing trades...\")\n",
    "        process_live()\n",
    "        print(\"   ‚úì Trades processed\\n\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"‚úÖ DATA PIPELINE COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nüí° Reload data in Section 2 to see updates\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Pipeline failed: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úì update_all_data() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cursor state for Goldsky (tracks scraping progress)\n",
    "cursor_state_file = Path(\"goldsky/cursor_state.json\")\n",
    "\n",
    "print(\"üìç Goldsky Cursor State:\")\n",
    "if cursor_state_file.exists():\n",
    "    import json\n",
    "    with open(cursor_state_file, 'r') as f:\n",
    "        cursor_state = json.load(f)\n",
    "    print(f\"   File: {cursor_state_file}\")\n",
    "    print(f\"   State: {cursor_state}\")\n",
    "    print(\"   ‚úì Cursor state exists - scraper will resume from last position\")\n",
    "else:\n",
    "    print(f\"   File: {cursor_state_file}\")\n",
    "    print(\"   ‚ö†Ô∏è  No cursor state found - first run will start from beginning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display pipeline file status\nimport os\n\nprint(\"üìÇ Data Pipeline Files:\")\n\nfiles_to_check = [\n    (\"data/markets.parquet\", \"Markets (main)\"),\n    (\"data/missing_markets.parquet\", \"Markets (missing)\"),\n    (\"goldsky/orderFilled.parquet\", \"Raw orders\"),\n    (\"processed/trades.parquet\", \"Processed trades\")\n]\n\nfor filepath, description in files_to_check:\n    path = Path(filepath)\n    if path.exists():\n        mod_time = datetime.fromtimestamp(path.stat().st_mtime)\n        age_hours = (datetime.now() - mod_time).total_seconds() / 3600\n        size_mb = path.stat().st_size / (1024**2)\n        print(f\"   ‚úì {description}\")\n        print(f\"      {filepath} ({size_mb:.1f} MB, {age_hours:.1f}h old)\")\n    else:\n        print(f\"   ‚ùå {description}\")\n        print(f\"      {filepath} (not found)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual data refresh (COMMENTED OUT - uncomment to run)\n",
    "print(\"üîÑ Manual Data Refresh:\")\n",
    "print(\"   Uncomment and run to update all data:\\n\")\n",
    "print(\"   # update_all_data()\")\n",
    "print(\"\\n   ‚ö†Ô∏è  This may take several minutes depending on data volume\")\n",
    "print(\"   ‚ö†Ô∏è  Ensure you have internet connectivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 5: MARKET ANALYSIS & EXPLORATION ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 markets by volume\n",
    "print(\"üèÜ Top 20 Markets by Volume:\\n\")\n",
    "\n",
    "top_markets = (\n",
    "    markets_df\n",
    "    .sort('volume', descending=True)\n",
    "    .head(20)\n",
    "    .select(['question', 'volume', 'createdAt'])\n",
    ")\n",
    "\n",
    "print(top_markets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market search configuration\n",
    "ANALYSIS_CONFIG = {\n",
    "    'market_keyword': 'Trump',  # Search keyword\n",
    "    'min_volume': 1_000_000,    # Minimum volume filter\n",
    "}\n",
    "\n",
    "print(f\"üîç Market Search Configuration:\")\n",
    "print(f\"   Keyword: '{ANALYSIS_CONFIG['market_keyword']}'\")\n",
    "print(f\"   Min Volume: ${ANALYSIS_CONFIG['min_volume']:,}\\n\")\n",
    "\n",
    "# Find matching markets\n",
    "candidate_markets = (\n",
    "    markets_df\n",
    "    .filter(pl.col('question').str.contains(ANALYSIS_CONFIG['market_keyword'], literal=False))\n",
    "    .filter(pl.col('volume') >= ANALYSIS_CONFIG['min_volume'])\n",
    "    .sort('volume', descending=True)\n",
    ")\n",
    "\n",
    "print(f\"Found {len(candidate_markets)} markets matching criteria:\")\n",
    "print(candidate_markets.select(['id', 'question', 'volume']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if markets_df is not None:\n    # Select target market (top result)\n    if len(candidate_markets) > 0:\n        TARGET_MARKET_ID = candidate_markets.row(0, named=True)['id']\n        TARGET_MARKET = markets_df.filter(pl.col('id') == TARGET_MARKET_ID).row(0, named=True)\n        \n        print(f\"\\nüéØ Selected Target Market:\")\n        print(f\"   ID: {TARGET_MARKET['id']}\")\n        print(f\"   Question: {TARGET_MARKET['question']}\")\n        print(f\"   Volume: ${TARGET_MARKET['volume']:,.0f}\")\n        print(f\"   Created: {TARGET_MARKET['createdAt']}\")\n    else:\n        print(\"\\n‚ö†Ô∏è  No markets found - adjust search criteria\")\n        TARGET_MARKET_ID = None\nelse:\n    print(\"‚ö†Ô∏è  Markets data not loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract trades for target market\n",
    "if TARGET_MARKET_ID:\n",
    "    market_trades = (\n",
    "        trades_df\n",
    "        .filter(pl.col('market_id') == TARGET_MARKET_ID)\n",
    "        .with_columns([\n",
    "            # Standardize price to token1 perspective\n",
    "            pl.when(pl.col('nonusdc_side') == 'token2')\n",
    "            .then(1 - pl.col('price'))\n",
    "            .otherwise(pl.col('price'))\n",
    "            .alias('price_standardized')\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Market Trade Statistics:\")\n",
    "    print(f\"   Total trades: {len(market_trades):,}\")\n",
    "    print(f\"   Price range: {market_trades['price_standardized'].min():.3f} - {market_trades['price_standardized'].max():.3f}\")\n",
    "    print(f\"   Avg trade size: ${market_trades['usd_amount'].mean():.2f}\")\n",
    "    print(f\"   Total volume: ${market_trades['usd_amount'].sum():,.0f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping - no target market selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maker-taker analysis\n",
    "if TARGET_MARKET_ID:\n",
    "    print(\"\\nüîÑ Maker-Taker Dynamics:\")\n",
    "    \n",
    "    # Analyze makers per transaction\n",
    "    taker_stats = (\n",
    "        market_trades\n",
    "        .group_by(\"transactionHash\")\n",
    "        .agg([\n",
    "            pl.n_unique(\"maker\").alias(\"num_makers\"),\n",
    "            pl.col(\"usd_amount\").sum().alias(\"total_volume\")\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    print(f\"   Avg makers per taker order: {taker_stats['num_makers'].mean():.2f}\")\n",
    "    print(f\"   Median makers per taker order: {taker_stats['num_makers'].median():.0f}\")\n",
    "    print(f\"   Max makers in single order: {taker_stats['num_makers'].max()}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping - no target market selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price and volume visualization\n",
    "if TARGET_MARKET_ID:\n",
    "    # Convert to pandas for plotting\n",
    "    market_trades_pd = market_trades.to_pandas()\n",
    "    market_trades_pd = market_trades_pd.set_index('timestamp').sort_index()\n",
    "    \n",
    "    # Create 2-panel visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "    \n",
    "    # Price scatter plot\n",
    "    ax1.scatter(market_trades_pd.index, market_trades_pd['price_standardized'], \n",
    "                alpha=0.3, s=10, c='steelblue')\n",
    "    ax1.set_ylabel('Price', fontsize=12)\n",
    "    ax1.set_title(f\"Price & Volume History: {TARGET_MARKET['question'][:80]}...\", fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Volume bar chart (hourly)\n",
    "    volume_hourly = market_trades_pd['usd_amount'].resample('1H').sum()\n",
    "    ax2.bar(volume_hourly.index, volume_hourly.values, color='green', alpha=0.6, width=0.04)\n",
    "    ax2.set_ylabel('Volume (USD)', fontsize=12)\n",
    "    ax2.set_xlabel('Time', fontsize=12)\n",
    "    ax2.set_title('Hourly Trading Volume', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Visualization complete\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping - no target market selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 6: BACKTRADER PLOTTING SETUP ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import backtrader plotting modules\n",
    "try:\n",
    "    from backtrader_plotting import Bokeh\n",
    "    from backtrader_plotting.schemes import Blackly, Tradimo\n",
    "    import backtrader as bt\n",
    "    \n",
    "    print(\"‚úì backtrader and backtrader_plotting imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Import error: {e}\")\n",
    "    print(\"   Install with: pip install backtrader backtrader_plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available plotting schemes\n",
    "print(\"üé® Available Plotting Schemes:\\n\")\n",
    "\n",
    "schemes_info = [\n",
    "    (\"Blackly\", \"Dark background theme with high contrast\"),\n",
    "    (\"Tradimo\", \"Professional light theme\"),\n",
    "]\n",
    "\n",
    "for scheme_name, description in schemes_info:\n",
    "    print(f\"   ‚Ä¢ {scheme_name}: {description}\")\n",
    "\n",
    "print(\"\\nüí° Usage example:\")\n",
    "print(\"   scheme = Blackly()\")\n",
    "print(\"   b = Bokeh(style='bar', plot_mode='single', scheme=scheme)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plotting configuration\n",
    "PLOT_CONFIG = {\n",
    "    'scheme': 'Blackly',        # Theme: 'Blackly' or 'Tradimo'\n",
    "    'style': 'bar',              # Style: 'bar', 'line', or 'candle'\n",
    "    'plot_mode': 'single',       # Mode: 'single' or 'tabs'\n",
    "    'output_dir': 'processed/',  # Directory for saved plots\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è  Plotting Configuration:\")\n",
    "for key, value in PLOT_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n‚úì Configuration ready for strategy backtesting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available analyzer tables\n",
    "print(\"üìä Available Backtrader Analyzers:\\n\")\n",
    "\n",
    "analyzers = [\n",
    "    (\"SharpeRatio\", \"Risk-adjusted return metric\"),\n",
    "    (\"DrawDown\", \"Maximum drawdown analysis\"),\n",
    "    (\"Returns\", \"Total and annualized returns\"),\n",
    "    (\"TradeAnalyzer\", \"Win/loss statistics\"),\n",
    "    (\"Calmar\", \"Calmar ratio (return/max drawdown)\"),\n",
    "    (\"VWR\", \"Variability Weighted Return\"),\n",
    "    (\"SQN\", \"System Quality Number\"),\n",
    "    (\"TimeReturn\", \"Period-based returns\"),\n",
    "]\n",
    "\n",
    "for analyzer_name, description in analyzers:\n",
    "    print(f\"   ‚Ä¢ {analyzer_name}\")\n",
    "    print(f\"     {description}\")\n",
    "\n",
    "print(\"\\nüí° Add to backtest with:\")\n",
    "print(\"   cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"‚úÖ BACKTRADER PLOTTING SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(\"   ‚Ä¢ Section 7: Add strategy implementation\")\n",
    "print(\"   ‚Ä¢ Section 8: Add backtesting framework\")\n",
    "print(\"   ‚Ä¢ Section 9: Add live trading capabilities\")\n",
    "print(\"\\nüí° All utilities are now integrated and ready to use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}