{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 0: COLAB REPOSITORY SETUP ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository configuration\n",
    "REPO_URL = \"https://github.com/Sageder/spread.git\"\n",
    "REPO_BRANCH = \"main\"\n",
    "\n",
    "# For private repositories, uncomment and set your GitHub token:\n",
    "# GITHUB_TOKEN = \"ghp_your_token_here\"  # Get from: https://github.com/settings/tokens\n",
    "# REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/Sageder/spread.git\"\n",
    "\n",
    "print(\"\ud83d\udd27 Repository Configuration:\")\n",
    "print(f\"   URL: {REPO_URL.replace(GITHUB_TOKEN, '***') if 'GITHUB_TOKEN' in dir() else REPO_URL}\")\n",
    "print(f\"   Branch: {REPO_BRANCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if already cloned\n",
    "if Path(\"spread\").exists():\n",
    "    print(\"\u2713 Repository already cloned\")\n",
    "    print(\"   Pulling latest changes...\")\n",
    "    !cd spread && git pull origin {REPO_BRANCH}\n",
    "else:\n",
    "    print(\"\ud83d\udce5 Cloning repository...\")\n",
    "    !git clone {REPO_URL}\n",
    "    \n",
    "    if not Path(\"spread\").exists():\n",
    "        print(\"\\n\u274c Clone failed!\")\n",
    "        print(\"\\nPossible reasons:\")\n",
    "        print(\"   1. Repository doesn't exist yet - push your local code to GitHub first\")\n",
    "        print(\"   2. Repository is private - add GitHub token in cell above\")\n",
    "        print(\"   3. Network error - check internet connection\")\n",
    "        print(\"\\n\ud83d\udca1 To push your code to GitHub:\")\n",
    "        print(\"   cd /path/to/spread\")\n",
    "        print(\"   git init\")\n",
    "        print(\"   git add .\")\n",
    "        print(\"   git commit -m 'Initial commit'\")\n",
    "        print(\"   git remote add origin https://github.com/sageder/spread.git\")\n",
    "        print(\"   git push -u origin main\")\n",
    "        raise Exception(\"Repository clone failed\")\n",
    "    \n",
    "    print(\"\u2713 Repository cloned\")\n",
    "\n",
    "# Change working directory to repo root\n",
    "%cd spread\n",
    "\n",
    "print(f\"\\n\u2705 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install project dependencies\n",
    "print(\"\ud83d\udce6 Installing dependencies...\")\n",
    "\n",
    "# Core dependencies from pyproject.toml\n",
    "!pip install -q pandas>=2.0.0 polars>=0.19.0 requests>=2.31.0 'gql[requests]>=3.4.0' flatten-json>=0.1.13\n",
    "\n",
    "# Additional Colab-specific dependencies\n",
    "!pip install -q google-cloud-storage matplotlib backtrader backtrader-plotting\n",
    "\n",
    "print(\"\u2705 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify imports work\n",
    "try:\n",
    "    from poly_utils.utils import get_markets, update_missing_tokens, PLATFORM_WALLETS\n",
    "    from update_utils.update_markets import update_markets\n",
    "    from update_utils.update_goldsky import update_goldsky\n",
    "    from update_utils.process_live import process_live\n",
    "    \n",
    "    print(\"\u2705 All utility modules imported successfully\")\n",
    "    print(f\"   \u2022 poly_utils: {len(PLATFORM_WALLETS)} platform wallets loaded\")\n",
    "    print(f\"   \u2022 update_utils: 3 pipeline functions available\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Import error: {e}\")\n",
    "    print(\"   Check that repository structure is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 1: PYTHON ENVIRONMENT SETUP ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "print(\"\u2713 Core libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Polars display settings\n",
    "pl.Config.set_tbl_rows(25)\n",
    "pl.Config.set_tbl_cols(-1)\n",
    "pl.Config.set_tbl_width_chars(1000)\n",
    "\n",
    "print(\"\u2713 Polars display configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\u2713 Matplotlib configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"\u2713 Autoreload enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\u2705 SETUP COMPLETE - Ready to load data\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 2: GCS DATA LOADING & VALIDATION ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS Authentication (for Google Colab)\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    \n",
    "    import os\n",
    "    os.environ['GCLOUD_PROJECT'] = 'alpine-charge-404612'\n",
    "    \n",
    "    print(\"\u2713 GCP authenticated\")\n",
    "except ImportError:\n",
    "    print(\"\u2139\ufe0f  Not running in Colab - skipping GCP auth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS Configuration\n",
    "import tarfile\n",
    "import zipfile\n",
    "import shutil\n",
    "import traceback\n",
    "from google.cloud import storage\n",
    "from google.api_core import retry\n",
    "\n",
    "# Config\n",
    "BUCKET_NAME = \"spread-eth-oxford\"\n",
    "GCS_PREFIX = \"polydata/\"\n",
    "GCS_RAW_ARCHIVE = \"archive.tar.xz\"\n",
    "WORK_DIR = \"data\"\n",
    "\n",
    "# Initialize GCS with retry\n",
    "try:\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    print(f\"\u2713 Connected to GCS bucket: {BUCKET_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f  GCS connection failed: {e}\")\n",
    "    storage_client = None\n",
    "    bucket = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_disk_space(required_gb=100):\n",
    "    \"\"\"Check if we have enough disk space\"\"\"\n",
    "    disk = shutil.disk_usage('.')\n",
    "    free_gb = disk.free / (1024**3)\n",
    "\n",
    "    print(f\"\ud83d\udcbe Disk space check:\")\n",
    "    print(f\"   Free: {free_gb:.1f} GB\")\n",
    "    print(f\"   Required: ~{required_gb} GB\")\n",
    "\n",
    "    if free_gb < required_gb:\n",
    "        print(f\"\\n\u274c ERROR: Not enough disk space!\")\n",
    "        print(f\"   Need {required_gb}GB, have {free_gb:.1f}GB\")\n",
    "        return False\n",
    "\n",
    "    print(f\"   \u2713 Sufficient space available\\n\")\n",
    "    return True\n",
    "\n",
    "def check_gcs_data():\n",
    "    \"\"\"Check if processed data exists in GCS\"\"\"\n",
    "    if not bucket:\n",
    "        return False\n",
    "    try:\n",
    "        blobs = [b for b in bucket.list_blobs(prefix=GCS_PREFIX) if b.name.endswith('.zip')]\n",
    "        return len(blobs) > 0\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  Warning: Could not check GCS: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"\u2713 GCS helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_gcs():\n",
    "    \"\"\"Download and extract zip archives from GCS with retry\"\"\"\n",
    "    if not bucket:\n",
    "        print(\"\u274c GCS not connected\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "        blobs = [b for b in bucket.list_blobs(prefix=GCS_PREFIX) if b.name.endswith('.zip')]\n",
    "        print(f\"\ud83d\udce5 Downloading {len(blobs)} zip archive(s) from GCS...\\n\")\n",
    "\n",
    "        for i, blob in enumerate(blobs, 1):\n",
    "            filename = blob.name.split('/')[-1]\n",
    "            zip_path = filename\n",
    "\n",
    "            # Skip if already extracted\n",
    "            expected_files = Path(WORK_DIR).glob(f\"{filename.replace('.zip', '')}*.parquet\")\n",
    "            if any(expected_files):\n",
    "                print(f\"[{i}/{len(blobs)}] {filename} - Already extracted, skipping\")\n",
    "                continue\n",
    "\n",
    "            # Reload blob to get metadata\n",
    "            blob.reload()\n",
    "            blob_size_mb = (blob.size or 0) / (1024**2)\n",
    "            print(f\"[{i}/{len(blobs)}] {filename} ({blob_size_mb:.1f} MB)...\", end=\" \")\n",
    "\n",
    "            # Download with retry\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    start = time.time()\n",
    "                    blob.download_to_filename(zip_path)\n",
    "                    elapsed = time.time() - start\n",
    "                    file_size_mb = os.path.getsize(zip_path) / (1024**2)\n",
    "                    speed = file_size_mb / elapsed if elapsed > 0 else 0\n",
    "                    print(f\"\u2713 ({speed:.1f} MB/s)\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        print(f\"\\n   Retry {attempt+1}/{max_retries}...\", end=\" \")\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        raise Exception(f\"Download failed after {max_retries} attempts: {e}\")\n",
    "\n",
    "            # Extract\n",
    "            print(f\"    Extracting...\", end=\" \")\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "                    zipf.extractall(WORK_DIR)\n",
    "                print(f\"\u2713\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n\u274c ERROR extracting {filename}: {e}\")\n",
    "                raise\n",
    "\n",
    "            # Remove zip\n",
    "            os.remove(zip_path)\n",
    "            print()\n",
    "\n",
    "        print(f\"\u2705 Downloaded and extracted to {WORK_DIR}/\\n\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c ERROR in download_from_gcs: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "print(\"\u2713 download_from_gcs() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main GCS data loading execution\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\ude80 DATA LOADER\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "success = False\n",
    "\n",
    "if check_gcs_data():\n",
    "    print(\"\u2705 Processed data found in GCS! Downloading...\\n\")\n",
    "    success = download_from_gcs()\n",
    "else:\n",
    "    print(\"\u2139\ufe0f  No processed data in GCS\")\n",
    "    print(\"   Place data files in data/ directory or run GCS upload process\")\n",
    "\n",
    "if success:\n",
    "    print(\"=\"*60)\n",
    "    print(\"\u2705 DATA READY!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Show files\n",
    "    print(f\"\\nFiles in {WORK_DIR}/:\")\n",
    "    parquet_files = sorted(Path(WORK_DIR).glob('*.parquet'))\n",
    "    total_size = 0\n",
    "    for f in parquet_files:\n",
    "        size_mb = f.stat().st_size / (1024**2)\n",
    "        total_size += size_mb\n",
    "        print(f\"  \ud83d\udcc4 {f.name} - {size_mb:.1f} MB\")\n",
    "\n",
    "    if parquet_files:\n",
    "        print(f\"\\nTotal: {len(parquet_files)} files, {total_size/1024:.2f} GB\")\n",
    "\n",
    "    disk = shutil.disk_usage('.')\n",
    "    print(f\"\ud83d\udcbe Disk: {disk.free/(1024**3):.1f} GB free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import poly_utils (already verified in Section 0)\n",
    "from poly_utils.utils import get_markets, update_missing_tokens, PLATFORM_WALLETS\n",
    "\n",
    "print(\"\u2713 poly_utils loaded from repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load markets using poly_utils\n",
    "print(\"\ud83d\udcca Loading markets...\")\n",
    "\n",
    "# Load markets from parquet (downloaded from GCS)\n",
    "markets_parquet = Path(f\"{WORK_DIR}/markets.parquet\")\n",
    "\n",
    "if markets_parquet.exists():\n",
    "    markets_df = pl.read_parquet(markets_parquet)\n",
    "    print(f\"\u2713 Markets loaded from parquet: {len(markets_df):,} markets\")\n",
    "    print(f\"   Volume range: ${markets_df['volume'].min():,.0f} - ${markets_df['volume'].max():,.0f}\")\n",
    "\n",
    "    # Check if datetime column needs parsing\n",
    "    if markets_df['createdAt'].dtype == pl.Utf8:\n",
    "        markets_df = markets_df.with_columns(\n",
    "            pl.col(\"createdAt\").str.to_datetime().alias(\"createdAt\")\n",
    "        )\n",
    "else:\n",
    "    print(f\"\u274c No markets data found\")\n",
    "    print(f\"   Expected: {markets_parquet}\")\n",
    "    print(f\"   Run data update pipeline in Section 4 to generate data\")\n",
    "    markets_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trades from processed directory\n",
    "print(\"\ud83d\udcc8 Loading trades...\")\n",
    "\n",
    "# Load trades from parquet files\n",
    "trades_files = sorted(Path(WORK_DIR).glob(\"trades*.parquet\"))\n",
    "\n",
    "if len(trades_files) > 0:\n",
    "    print(f\"\ud83d\udcc8 Loading {len(trades_files)} trade file(s)...\")\n",
    "\n",
    "    if len(trades_files) == 1:\n",
    "        trades_df = pl.read_parquet(trades_files[0])\n",
    "    else:\n",
    "        # Multiple files - use lazy scanning for memory efficiency\n",
    "        print(\"   Using memory-efficient lazy loading...\")\n",
    "        trades_df = pl.concat([\n",
    "            pl.scan_parquet(f) for f in trades_files\n",
    "        ]).collect(streaming=True)\n",
    "\n",
    "    # Check if timestamp needs parsing\n",
    "    if trades_df['timestamp'].dtype == pl.Utf8:\n",
    "        trades_df = trades_df.with_columns(\n",
    "            pl.col(\"timestamp\").str.to_datetime().alias(\"timestamp\")\n",
    "        )\n",
    "\n",
    "    print(f\"\u2713 Trades loaded: {len(trades_df):,} trades\")\n",
    "    print(f\"   Total volume: ${trades_df['usd_amount'].sum()/1e9:.2f}B\")\n",
    "    print(f\"   Date range: {trades_df['timestamp'].min()} to {trades_df['timestamp'].max()}\")\n",
    "    print(f\"   Unique markets: {trades_df['market_id'].n_unique():,}\")\n",
    "else:\n",
    "    print(\"\u274c No trades data found\")\n",
    "    print(f\"   Expected: {WORK_DIR}/trades*.parquet\")\n",
    "    print(f\"   Run data update pipeline in Section 4 to generate data\")\n",
    "    trades_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if markets_df is not None and trades_df is not None:\n",
    "    # Validation: Check for missing markets in trades\n",
    "    print(\"\ud83d\udd0d Validating data...\")\n",
    "    \n",
    "    missing_market_ids = set(trades_df['market_id'].unique()) - set(markets_df['id'].unique())\n",
    "    \n",
    "    if missing_market_ids:\n",
    "        print(f\"\u26a0\ufe0f  Warning: {len(missing_market_ids)} market IDs in trades not found in markets\")\n",
    "        print(f\"   Sample missing IDs: {list(missing_market_ids)[:5]}\")\n",
    "        print(f\"   Consider running update_missing_tokens() in Section 3\")\n",
    "    else:\n",
    "        print(\"\u2705 All trade markets found in markets dataset\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Cannot validate - data not loaded\")\n",
    "    missing_market_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trades_df is not None:\n",
    "    # Validation: Data freshness check\n",
    "    last_trade_time = trades_df['timestamp'].max()\n",
    "    hours_since_last = (datetime.now() - last_trade_time).total_seconds() / 3600\n",
    "    \n",
    "    print(f\"\\n\u23f0 Data freshness:\")\n",
    "    print(f\"   Last trade: {last_trade_time}\")\n",
    "    print(f\"   Hours ago: {hours_since_last:.1f}h\")\n",
    "    \n",
    "    if hours_since_last > 24:\n",
    "        print(\"   \u26a0\ufe0f  Data is stale - consider running update pipeline in Section 4\")\n",
    "    else:\n",
    "        print(\"   \u2705 Data is fresh\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\u2705 DATA VALIDATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Cannot check freshness - trades data not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 3: POLYUTILS - MARKET MANAGEMENT ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display platform wallet addresses\n",
    "print(\"\ud83d\udcbc Platform Wallets Tracked:\")\n",
    "for i, wallet in enumerate(PLATFORM_WALLETS, 1):\n",
    "    print(f\"   {i}. {wallet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if markets_df is not None:\n",
    "    # Example: Filter markets by volume and keyword\n",
    "    print(\"\ud83d\udd0d Market Filtering Examples:\\n\")\n",
    "    \n",
    "    # High volume markets\n",
    "    high_volume = markets_df.filter(pl.col('volume') >= 1_000_000).sort('volume', descending=True)\n",
    "    print(f\"1. High volume markets (>=1M): {len(high_volume):,}\")\n",
    "    print(high_volume.select(['question', 'volume']).head(5))\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "    \n",
    "    # Search by keyword\n",
    "    keyword = \"Trump\"\n",
    "    keyword_markets = markets_df.filter(\n",
    "        pl.col('question').str.contains(keyword, literal=False)\n",
    "    ).sort('volume', descending=True)\n",
    "    print(f\"2. Markets containing '{keyword}': {len(keyword_markets):,}\")\n",
    "    print(keyword_markets.select(['question', 'volume']).head(5))\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Markets data not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Update missing tokens (COMMENTED OUT - run only when needed)\n",
    "print(\"\ud83d\udcdd Update Missing Tokens Example:\")\n",
    "print(\"   Use this when you have market IDs in trades but not in markets dataset\")\n",
    "print(\"   Uncomment and run the code below:\\n\")\n",
    "\n",
    "print(\"   # missing_ids = ['market_id_1', 'market_id_2', ...]\")\n",
    "print(\"   # update_missing_tokens(missing_ids, output_file='missing_markets.csv')\")\n",
    "print(\"   # Then reload markets with get_markets()\")\n",
    "\n",
    "if missing_market_ids:\n",
    "    print(f\"\\n   \ud83d\udca1 You currently have {len(missing_market_ids)} missing markets\")\n",
    "    print(f\"   Sample IDs to update: {list(missing_market_ids)[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display market schema\n",
    "print(\"\ud83d\udccb Market Data Schema:\")\n",
    "print(f\"   Total columns: {len(markets_df.columns)}\")\n",
    "print(f\"\\n   Columns: {', '.join(markets_df.columns)}\")\n",
    "print(f\"\\n   Sample row:\")\n",
    "print(markets_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 4: UPDATE UTILS - DATA PIPELINE ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import update_utils modules\n",
    "from update_utils.update_markets import update_markets\n",
    "from update_utils.update_goldsky import update_goldsky\n",
    "from update_utils.process_live import process_live\n",
    "\n",
    "print(\"\u2713 update_utils modules imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wrapper function for complete data update\n",
    "def update_all_data():\n",
    "    \"\"\"\n",
    "    Run complete data pipeline update:\n",
    "    1. Update markets from Polymarket API\n",
    "    2. Scrape order events from Goldsky subgraph\n",
    "    3. Process raw orders into structured trades\n",
    "    \"\"\"\n",
    "    print(\"\ud83d\udd04 Starting data update pipeline...\\n\")\n",
    "    \n",
    "    try:\n",
    "        print(\"1\ufe0f\u20e3 Updating markets from Polymarket API...\")\n",
    "        update_markets()\n",
    "        print(\"   \u2713 Markets updated\\n\")\n",
    "        \n",
    "        print(\"2\ufe0f\u20e3 Scraping order events from Goldsky...\")\n",
    "        update_goldsky()\n",
    "        print(\"   \u2713 Orders scraped\\n\")\n",
    "        \n",
    "        print(\"3\ufe0f\u20e3 Processing trades...\")\n",
    "        process_live()\n",
    "        print(\"   \u2713 Trades processed\\n\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"\u2705 DATA PIPELINE COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\n\ud83d\udca1 Reload data in Section 2 to see updates\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Pipeline failed: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"\u2713 update_all_data() function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cursor state for Goldsky (tracks scraping progress)\n",
    "cursor_state_file = Path(\"goldsky/cursor_state.json\")\n",
    "\n",
    "print(\"\ud83d\udccd Goldsky Cursor State:\")\n",
    "if cursor_state_file.exists():\n",
    "    import json\n",
    "    with open(cursor_state_file, 'r') as f:\n",
    "        cursor_state = json.load(f)\n",
    "    print(f\"   File: {cursor_state_file}\")\n",
    "    print(f\"   State: {cursor_state}\")\n",
    "    print(\"   \u2713 Cursor state exists - scraper will resume from last position\")\n",
    "else:\n",
    "    print(f\"   File: {cursor_state_file}\")\n",
    "    print(\"   \u26a0\ufe0f  No cursor state found - first run will start from beginning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display pipeline file status\n",
    "import os\n",
    "\n",
    "print(\"\ud83d\udcc2 Data Pipeline Files:\")\n",
    "\n",
    "files_to_check = [\n",
    "    (\"data/markets.parquet\", \"Markets (main)\"),\n",
    "    (\"data/missing_markets.parquet\", \"Markets (missing)\"),\n",
    "    (\"goldsky/orderFilled.parquet\", \"Raw orders\"),\n",
    "    (\"processed/trades.parquet\", \"Processed trades\")\n",
    "]\n",
    "\n",
    "for filepath, description in files_to_check:\n",
    "    path = Path(filepath)\n",
    "    if path.exists():\n",
    "        mod_time = datetime.fromtimestamp(path.stat().st_mtime)\n",
    "        age_hours = (datetime.now() - mod_time).total_seconds() / 3600\n",
    "        size_mb = path.stat().st_size / (1024**2)\n",
    "        print(f\"   \u2713 {description}\")\n",
    "        print(f\"      {filepath} ({size_mb:.1f} MB, {age_hours:.1f}h old)\")\n",
    "    else:\n",
    "        print(f\"   \u274c {description}\")\n",
    "        print(f\"      {filepath} (not found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual data refresh (COMMENTED OUT - uncomment to run)\n",
    "print(\"\ud83d\udd04 Manual Data Refresh:\")\n",
    "print(\"   Uncomment and run to update all data:\\n\")\n",
    "print(\"   # update_all_data()\")\n",
    "print(\"\\n   \u26a0\ufe0f  This may take several minutes depending on data volume\")\n",
    "print(\"   \u26a0\ufe0f  Ensure you have internet connectivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 5: MARKET ANALYSIS & EXPLORATION ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 markets by volume\n",
    "print(\"\ud83c\udfc6 Top 20 Markets by Volume:\\n\")\n",
    "\n",
    "top_markets = (\n",
    "    markets_df\n",
    "    .sort('volume', descending=True)\n",
    "    .head(20)\n",
    "    .select(['question', 'volume', 'createdAt'])\n",
    ")\n",
    "\n",
    "print(top_markets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market search configuration\n",
    "ANALYSIS_CONFIG = {\n",
    "    'market_keyword': 'Trump',  # Search keyword\n",
    "    'min_volume': 1_000_000,    # Minimum volume filter\n",
    "}\n",
    "\n",
    "print(f\"\ud83d\udd0d Market Search Configuration:\")\n",
    "print(f\"   Keyword: '{ANALYSIS_CONFIG['market_keyword']}'\")\n",
    "print(f\"   Min Volume: ${ANALYSIS_CONFIG['min_volume']:,}\\n\")\n",
    "\n",
    "# Find matching markets\n",
    "candidate_markets = (\n",
    "    markets_df\n",
    "    .filter(pl.col('question').str.contains(ANALYSIS_CONFIG['market_keyword'], literal=False))\n",
    "    .filter(pl.col('volume') >= ANALYSIS_CONFIG['min_volume'])\n",
    "    .sort('volume', descending=True)\n",
    ")\n",
    "\n",
    "print(f\"Found {len(candidate_markets)} markets matching criteria:\")\n",
    "print(candidate_markets.select(['id', 'question', 'volume']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if markets_df is not None:\n",
    "    # Select target market (top result)\n",
    "    if len(candidate_markets) > 0:\n",
    "        TARGET_MARKET_ID = candidate_markets.row(0, named=True)['id']\n",
    "        TARGET_MARKET = markets_df.filter(pl.col('id') == TARGET_MARKET_ID).row(0, named=True)\n",
    "        \n",
    "        print(f\"\\n\ud83c\udfaf Selected Target Market:\")\n",
    "        print(f\"   ID: {TARGET_MARKET['id']}\")\n",
    "        print(f\"   Question: {TARGET_MARKET['question']}\")\n",
    "        print(f\"   Volume: ${TARGET_MARKET['volume']:,.0f}\")\n",
    "        print(f\"   Created: {TARGET_MARKET['createdAt']}\")\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f  No markets found - adjust search criteria\")\n",
    "        TARGET_MARKET_ID = None\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Markets data not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract trades for target market\n",
    "if TARGET_MARKET_ID:\n",
    "    market_trades = (\n",
    "        trades_df\n",
    "        .filter(pl.col('market_id') == TARGET_MARKET_ID)\n",
    "        .with_columns([\n",
    "            # Standardize price to token1 perspective\n",
    "            pl.when(pl.col('nonusdc_side') == 'token2')\n",
    "            .then(1 - pl.col('price'))\n",
    "            .otherwise(pl.col('price'))\n",
    "            .alias('price_standardized')\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Market Trade Statistics:\")\n",
    "    print(f\"   Total trades: {len(market_trades):,}\")\n",
    "    print(f\"   Price range: {market_trades['price_standardized'].min():.3f} - {market_trades['price_standardized'].max():.3f}\")\n",
    "    print(f\"   Avg trade size: ${market_trades['usd_amount'].mean():.2f}\")\n",
    "    print(f\"   Total volume: ${market_trades['usd_amount'].sum():,.0f}\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  Skipping - no target market selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maker-taker analysis\n",
    "if TARGET_MARKET_ID:\n",
    "    print(\"\\n\ud83d\udd04 Maker-Taker Dynamics:\")\n",
    "    \n",
    "    # Analyze makers per transaction\n",
    "    taker_stats = (\n",
    "        market_trades\n",
    "        .group_by(\"transactionHash\")\n",
    "        .agg([\n",
    "            pl.n_unique(\"maker\").alias(\"num_makers\"),\n",
    "            pl.col(\"usd_amount\").sum().alias(\"total_volume\")\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    print(f\"   Avg makers per taker order: {taker_stats['num_makers'].mean():.2f}\")\n",
    "    print(f\"   Median makers per taker order: {taker_stats['num_makers'].median():.0f}\")\n",
    "    print(f\"   Max makers in single order: {taker_stats['num_makers'].max()}\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  Skipping - no target market selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price and volume visualization\n",
    "if TARGET_MARKET_ID:\n",
    "    # Convert to pandas for plotting\n",
    "    market_trades_pd = market_trades.to_pandas()\n",
    "    market_trades_pd = market_trades_pd.set_index('timestamp').sort_index()\n",
    "    \n",
    "    # Create 2-panel visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "    \n",
    "    # Price scatter plot\n",
    "    ax1.scatter(market_trades_pd.index, market_trades_pd['price_standardized'], \n",
    "                alpha=0.3, s=10, c='steelblue')\n",
    "    ax1.set_ylabel('Price', fontsize=12)\n",
    "    ax1.set_title(f\"Price & Volume History: {TARGET_MARKET['question'][:80]}...\", fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Volume bar chart (hourly)\n",
    "    volume_hourly = market_trades_pd['usd_amount'].resample('1H').sum()\n",
    "    ax2.bar(volume_hourly.index, volume_hourly.values, color='green', alpha=0.6, width=0.04)\n",
    "    ax2.set_ylabel('Volume (USD)', fontsize=12)\n",
    "    ax2.set_xlabel('Time', fontsize=12)\n",
    "    ax2.set_title('Hourly Trading Volume', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\u2705 Visualization complete\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f  Skipping - no target market selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === SECTION 6: BACKTRADER PLOTTING SETUP ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import backtrader plotting modules\n",
    "try:\n",
    "    from backtrader_plotting import Bokeh\n",
    "    from backtrader_plotting.schemes import Blackly, Tradimo\n",
    "    import backtrader as bt\n",
    "    \n",
    "    print(\"\u2713 backtrader and backtrader_plotting imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u26a0\ufe0f  Import error: {e}\")\n",
    "    print(\"   Install with: pip install backtrader backtrader_plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available plotting schemes\n",
    "print(\"\ud83c\udfa8 Available Plotting Schemes:\\n\")\n",
    "\n",
    "schemes_info = [\n",
    "    (\"Blackly\", \"Dark background theme with high contrast\"),\n",
    "    (\"Tradimo\", \"Professional light theme\"),\n",
    "]\n",
    "\n",
    "for scheme_name, description in schemes_info:\n",
    "    print(f\"   \u2022 {scheme_name}: {description}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Usage example:\")\n",
    "print(\"   scheme = Blackly()\")\n",
    "print(\"   b = Bokeh(style='bar', plot_mode='single', scheme=scheme)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plotting configuration\n",
    "PLOT_CONFIG = {\n",
    "    'scheme': 'Blackly',        # Theme: 'Blackly' or 'Tradimo'\n",
    "    'style': 'bar',              # Style: 'bar', 'line', or 'candle'\n",
    "    'plot_mode': 'single',       # Mode: 'single' or 'tabs'\n",
    "    'output_dir': 'processed/',  # Directory for saved plots\n",
    "}\n",
    "\n",
    "print(\"\u2699\ufe0f  Plotting Configuration:\")\n",
    "for key, value in PLOT_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n\u2713 Configuration ready for strategy backtesting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available analyzer tables\n",
    "print(\"\ud83d\udcca Available Backtrader Analyzers:\\n\")\n",
    "\n",
    "analyzers = [\n",
    "    (\"SharpeRatio\", \"Risk-adjusted return metric\"),\n",
    "    (\"DrawDown\", \"Maximum drawdown analysis\"),\n",
    "    (\"Returns\", \"Total and annualized returns\"),\n",
    "    (\"TradeAnalyzer\", \"Win/loss statistics\"),\n",
    "    (\"Calmar\", \"Calmar ratio (return/max drawdown)\"),\n",
    "    (\"VWR\", \"Variability Weighted Return\"),\n",
    "    (\"SQN\", \"System Quality Number\"),\n",
    "    (\"TimeReturn\", \"Period-based returns\"),\n",
    "]\n",
    "\n",
    "for analyzer_name, description in analyzers:\n",
    "    print(f\"   \u2022 {analyzer_name}\")\n",
    "    print(f\"     {description}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Add to backtest with:\")\n",
    "print(\"   cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\u2705 BACKTRADER PLOTTING SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n\ud83d\udcdd Next Steps:\")\n",
    "print(\"   \u2022 Section 7: Add strategy implementation\")\n",
    "print(\"   \u2022 Section 8: Add backtesting framework\")\n",
    "print(\"   \u2022 Section 9: Add live trading capabilities\")\n",
    "print(\"\\n\ud83d\udca1 All utilities are now integrated and ready to use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}